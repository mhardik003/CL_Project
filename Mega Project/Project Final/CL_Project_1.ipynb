{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PROJECT SUBMISSION A BY \n",
        "  * HARDIK MITTAL\n",
        "  * SANKALP BAHAD\n",
        "  * SUYASH SETHIA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Google colab python notebook link : https://colab.research.google.com/drive/1GQTX4NUljZtXFM0bmc2Ajp6E-IPgtVJt?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-biCMEYLBnr"
      },
      "source": [
        "## IMPORTING THE REQUIRED LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ngh48UWfqwd",
        "outputId": "354395d5-90bb-4cb6-980d-17588c9cc1c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.7.7)\n",
            "Requirement already satisfied: setuptools in /home/hardk/.local/lib/python3.10/site-packages (from spacy) (62.3.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.22.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.64.0)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.0.15)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/hardk/.local/lib/python3.10/site-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.6.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (4.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/hardk/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.5)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/lib/python3/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: stanza in /home/hardk/.local/lib/python3.10/site-packages (1.4.0)\n",
            "Requirement already satisfied: requests in /home/hardk/.local/lib/python3.10/site-packages (from stanza) (2.27.1)\n",
            "Requirement already satisfied: protobuf in /usr/lib/python3/dist-packages (from stanza) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stanza) (1.22.3)\n",
            "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from stanza) (1.16.0)\n",
            "Requirement already satisfied: transformers in /home/hardk/.local/lib/python3.10/site-packages (from stanza) (4.20.0)\n",
            "Requirement already satisfied: emoji in /home/hardk/.local/lib/python3.10/site-packages (from stanza) (1.7.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stanza) (4.64.0)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (1.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (4.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->stanza) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/hardk/.local/lib/python3.10/site-packages (from requests->stanza) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->stanza) (1.26.5)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->stanza) (2022.4.24)\n",
            "Requirement already satisfied: filelock in /home/hardk/.local/lib/python3.10/site-packages (from transformers->stanza) (3.7.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/hardk/.local/lib/python3.10/site-packages (from transformers->stanza) (0.7.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->stanza) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/hardk/.local/lib/python3.10/site-packages (from transformers->stanza) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/hardk/.local/lib/python3.10/site-packages (from transformers->stanza) (0.12.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=20.0->transformers->stanza) (2.4.7)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json: 154kB [00:00, 6.55MB/s]                    \n",
            "2022-06-17 13:16:02 INFO: Downloading default packages for language: hi (Hindi)...\n",
            "2022-06-17 13:16:02 INFO: File exists: /home/hardk/stanza_resources/hi/default.zip\n",
            "2022-06-17 13:16:03 INFO: Finished downloading models and saved to /home/hardk/stanza_resources.\n"
          ]
        }
      ],
      "source": [
        "import re #importing regex package (for using the searching operation)\n",
        "import sys #just added coz it was aded earlier as well :0\n",
        "\n",
        "#from google.colab import files #to import/upload the teams_1.txt\n",
        "\n",
        "import numpy as np #not required yet\n",
        "import scipy as sio \n",
        "import pandas as pd #not required yet\n",
        "import matplotlib.pyplot as plt #not required yet\n",
        "\n",
        "#FOR THE STEMMER\n",
        "!pip3 install spacy \n",
        "import spacy\n",
        "from spacy.lang.hi import Hindi\n",
        "\n",
        "#from wordsDict import *\n",
        "!pip3 install stanza\n",
        "import stanza\n",
        "stanza.download('hi')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hGW_Micxtg7"
      },
      "source": [
        "## CLONING THE PROVIDED PARSER (ISCNLP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhrsABXQf821",
        "outputId": "4d9642f4-58ed-4ded-8a42-9504396b34dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n!rm -r pos-tagger #so that at every call new postagger is created\\n!git clone https://irshadbhat@bitbucket.org/iscnlp/pos-tagger.git  #cloning the pos tagger as mentioned in the documentation of iscnlp\\n\\n!cd pos-tagger\\n!sudo python3 setup.py install\\n!cd ..\\n\\n!rm -r parser #so that everytime it clones again, can be removed too\\n!git clone https://irshadbhat@bitbucket.org/iscnlp/parser.git #to clone the parser \\n!cd parser\\n!sudo python3 setup.py install #installing the hindi parser\\n!cd .. # to back to the main directorynlp = Hindi()\\n# doc = nlp(str)\\n# for token in doc:\\n#   print(token.text,\"->\", token.norm_)\\n\\n\\n#ls\\n'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "## USE THESE COMMANDS MANUALLY IN THE TERMINAL\n",
        "'''\n",
        "!rm -r pos-tagger #so that at every call new postagger is created\n",
        "!git clone https://irshadbhat@bitbucket.org/iscnlp/pos-tagger.git  #cloning the pos tagger as mentioned in the documentation of iscnlp\n",
        "\n",
        "!cd pos-tagger\n",
        "!sudo python3 setup.py install\n",
        "!cd ..\n",
        "\n",
        "!rm -r parser #so that everytime it clones again, can be removed too\n",
        "!git clone https://irshadbhat@bitbucket.org/iscnlp/parser.git #to clone the parser \n",
        "!cd parser\n",
        "!sudo python3 setup.py install #installing the hindi parser\n",
        "!cd .. # to back to the main directorynlp = Hindi()\n",
        "# doc = nlp(str)\n",
        "# for token in doc:\n",
        "#   print(token.text,\"->\", token.norm_)\n",
        "\n",
        "\n",
        "#ls\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9pnxyQux-L6"
      },
      "source": [
        "## IMPORTING LIBRAIRES FROM THE PROVIDED PARSER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oNIIrMS7qpv9"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals\n",
        "from isc_parser import Parser\n",
        "parser = Parser(lang='hin')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSuEN89SW_UR"
      },
      "source": [
        "## STEMMER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "u_0TbfT5W37S"
      },
      "outputs": [],
      "source": [
        "words_dict  = { \"तैराक\":\"तैर\",\n",
        "              \"चालाक\":\"चाल\",\n",
        "              \"कूलाक\":\"कूल\",\n",
        "              \"बेलन\":\"बेल\",\n",
        "              \"मिलाप\":\"मिल\",\n",
        "              \"चुपचाप\": \"चुप\",\n",
        "              \"निकास\":\"निकस\",\n",
        "              \"लुकास\":\"लुक\",\n",
        "              }\n",
        "\n",
        "suffixes = {\n",
        "    1: [\"ो\", \"े\", \"ू\", \"ु\", \"ी\", \"ि\", \"ा\"],  \n",
        "          2: [\"तृ\",\"ान\",\"ैत\",\"ने\",\"ाऊ\",\"ाव\",\"कर\", \"ाओ\", \"िए\", \"ाई\", \"ाए\", \"नी\", \"ना\", \"ते\", \"ीं\", \"ती\",\n",
        "              \"ता\", \"ाँ\", \"ां\", \"ों\", \"ें\",\"ीय\", \"ति\",\"या\", \"पन\", \"पा\",\"ित\",\"ीन\",\"लु\",\"यत\",\"वट\",\"लू\"],     \n",
        "          3: [\"ेरा\",\"त्व\",\"नीय\",\"ौनी\",\"ौवल\",\"ौती\",\"ौता\",\"ापा\",\"वास\",\"हास\",\"काल\",\"पान\",\"न्त\",\"ौना\",\"सार\",\"पोश\",\"नाक\",\n",
        "              \"ियल\",\"ैया\", \"ौटी\",\"ावा\",\"ाहट\",\"िया\",\"हार\", \"ाकर\", \"ाइए\", \"ाईं\", \"ाया\", \"ेगी\", \"वान\", \"बीन\",\n",
        "              \"ेगा\", \"ोगी\", \"ोगे\", \"ाने\", \"ाना\", \"ाते\", \"ाती\", \"ाता\", \"तीं\", \"ाओं\", \"ाएं\", \"ुओं\", \"ुएं\", \"ुआं\",\"कला\",\"िमा\",\"कार\",\n",
        "              \"गार\", \"दान\",\"खोर\"],     \n",
        "          4: [\"ावास\",\"कलाप\",\"हारा\",\"तव्य\",\"वैया\", \"वाला\", \"ाएगी\", \"ाएगा\", \"ाओगी\", \"ाओगे\", \n",
        "              \"एंगी\", \"ेंगी\", \"एंगे\", \"ेंगे\", \"ूंगी\", \"ूंगा\", \"ातीं\", \"नाओं\", \"नाएं\", \"ताओं\", \"ताएं\", \"ियाँ\", \"ियों\", \"ियां\",\n",
        "              \"त्वा\",\"तव्य\",\"कल्प\",\"िष्ठ\",\"जादा\",\"क्कड़\"],     \n",
        "          5: [\"ाएंगी\", \"ाएंगे\", \"ाऊंगी\", \"ाऊंगा\", \"ाइयाँ\", \"ाइयों\", \"ाइयां\", \"अक्कड़\",\"तव्य:\",\"निष्ठ\"],\n",
        "}\n",
        "\n",
        "special_suffixes = [\"र्\", \"ज्य\",\"त्य\"]\n",
        "dict_special_suffixes = {\"र्\":\"ृ\",\n",
        "                        \"ज्य\":\"ज्\",\n",
        "                        \"त्य\":\"त्\"}\n",
        "\n",
        "def hi_stem(word, clean=False,chars=None):\n",
        "  if clean == True:\n",
        "      word = clean_text(word, chars)\n",
        "  \n",
        "  ans = word\n",
        "  bl = False\n",
        "  \n",
        "  if word in words_dict.keys():\n",
        "      return words_dict[word]\n",
        "  \n",
        "  for L in 5, 4, 3, 2, 1:\n",
        "      if len(word) > L + 1:\n",
        "          for suf in suffixes[L]:\n",
        "              if word.endswith(suf):\n",
        "                  ans = word[:-L]\n",
        "                  bl =True\n",
        "      if bl == True:\n",
        "          break\n",
        "                  \n",
        "  if bl == True:\n",
        "      for suf in suffixes[1]:\n",
        "          if ans.endswith(suf):\n",
        "              # use case - गानेवाला\n",
        "              ans = hi_stem(ans)\n",
        "  \n",
        "  for suf in special_suffixes:\n",
        "      if ans.endswith(suf):\n",
        "          l = len(suf)\n",
        "          ans = ans[:-l]\n",
        "          ans += dict_special_suffixes[suf]\n",
        "\n",
        "  return ans\n",
        "\n",
        "def clean_text(text, chars=None):\n",
        "  if chars == None:        \n",
        "      text = re.sub(r\"[()\\\"#/@;:<>{}`+=~|!?,']\", \"\", text)\n",
        "  else:\n",
        "      text = re.sub(r\"[\" +chars+ \"()\\\"#/@;:<>{}`+=~|!?,']\", \"\", text)\n",
        "  return text\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3M8UGXfmU8LX"
      },
      "source": [
        "## Finds first verb in the text per line present just before the \"कि\" and stems it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Schx5-aYfbEg",
        "outputId": "f927c830-2971-4555-c13a-c4a73fcfd7d3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json: 154kB [00:00, 2.53MB/s]                    \n",
            "2022-06-17 13:16:08 INFO: Loading these models for language: hi (Hindi):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | hdtb    |\n",
            "| pos       | hdtb    |\n",
            "| lemma     | hdtb    |\n",
            "| depparse  | hdtb    |\n",
            "=======================\n",
            "\n",
            "2022-06-17 13:16:08 INFO: Use device: gpu\n",
            "2022-06-17 13:16:08 INFO: Loading: tokenize\n",
            "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:145: UserWarning: \n",
            "NVIDIA GeForce RTX 3060 Laptop GPU with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
            "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
            "If you want to use the NVIDIA GeForce RTX 3060 Laptop GPU GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
            "\n",
            "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m/home/hardk/AMNESIA/Sem 2/CL-1/Mega Project/Resources/hardik/CL_Project_1.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hardk/AMNESIA/Sem%202/CL-1/Mega%20Project/Resources/hardik/CL_Project_1.ipynb#ch0000015?line=0'>1</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39moutput.txt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hardk/AMNESIA/Sem%202/CL-1/Mega%20Project/Resources/hardik/CL_Project_1.ipynb#ch0000015?line=1'>2</a>\u001b[0m hi_nlp \u001b[39m=\u001b[39m stanza\u001b[39m.\u001b[39;49mPipeline(\u001b[39m'\u001b[39;49m\u001b[39mhi\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hardk/AMNESIA/Sem%202/CL-1/Mega%20Project/Resources/hardik/CL_Project_1.ipynb#ch0000015?line=2'>3</a>\u001b[0m c \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m  \u001b[39m# line counter\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hardk/AMNESIA/Sem%202/CL-1/Mega%20Project/Resources/hardik/CL_Project_1.ipynb#ch0000015?line=4'>5</a>\u001b[0m pattern \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m कि \u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# added the spaces so that other words having \"ki\" are also not get used\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stanza/pipeline/core.py:263\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, lang, dir, package, processors, logging_level, verbose, use_gpu, model_dir, download_method, resources_url, resources_branch, resources_version, proxies, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m logger\u001b[39m.\u001b[39mdebug(curr_processor_config)\n\u001b[1;32m    261\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m     \u001b[39m# try to build processor, throw an exception if there is a requirements issue\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessors[processor_name] \u001b[39m=\u001b[39m NAME_TO_PROCESSOR_CLASS[processor_name](config\u001b[39m=\u001b[39;49mcurr_processor_config,\n\u001b[1;32m    264\u001b[0m                                                                               pipeline\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    265\u001b[0m                                                                               use_gpu\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muse_gpu)\n\u001b[1;32m    266\u001b[0m \u001b[39mexcept\u001b[39;00m ProcessorRequirementsException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    267\u001b[0m     \u001b[39m# if there was a requirements issue, add it to list which will be printed at end\u001b[39;00m\n\u001b[1;32m    268\u001b[0m     pipeline_reqs_exceptions\u001b[39m.\u001b[39mappend(e)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stanza/pipeline/processor.py:159\u001b[0m, in \u001b[0;36mUDProcessor.__init__\u001b[0;34m(self, config, pipeline, use_gpu)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vocab \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_variant\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 159\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_up_model(config, pipeline, use_gpu)\n\u001b[1;32m    161\u001b[0m \u001b[39m# build the final config for the processor\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_up_final_config(config)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stanza/pipeline/tokenize_processor.py:40\u001b[0m, in \u001b[0;36mTokenizeProcessor._set_up_model\u001b[0;34m(self, config, pipeline, use_gpu)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trainer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 40\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trainer \u001b[39m=\u001b[39m Trainer(model_file\u001b[39m=\u001b[39;49mconfig[\u001b[39m'\u001b[39;49m\u001b[39mmodel_path\u001b[39;49m\u001b[39m'\u001b[39;49m], use_cuda\u001b[39m=\u001b[39;49muse_gpu)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stanza/models/tokenization/trainer.py:30\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, args, vocab, lexicon, dictionary, model_file, use_cuda)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[39mif\u001b[39;00m use_cuda:\n\u001b[0;32m---> 30\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mcuda()\n\u001b[1;32m     31\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m     32\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:688\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    672\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \n\u001b[1;32m    674\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 688\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(\u001b[39mlambda\u001b[39;49;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    577\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 578\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    580\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    581\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    583\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:189\u001b[0m, in \u001b[0;36mRNNBase._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights \u001b[39m=\u001b[39m [(\u001b[39mlambda\u001b[39;00m wn: \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, wn) \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, wn) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)(wn) \u001b[39mfor\u001b[39;00m wn \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights_names]\n\u001b[1;32m    188\u001b[0m \u001b[39m# Flattens params (on CUDA)\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflatten_parameters()\n\u001b[1;32m    191\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:175\u001b[0m, in \u001b[0;36mRNNBase.flatten_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj_size \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    174\u001b[0m     num_weights \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 175\u001b[0m torch\u001b[39m.\u001b[39;49m_cudnn_rnn_flatten_weight(\n\u001b[1;32m    176\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, num_weights,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_size, rnn\u001b[39m.\u001b[39;49mget_cudnn_mode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode),\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhidden_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproj_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first, \u001b[39mbool\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional))\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
          ]
        }
      ],
      "source": [
        "output = open(\"output.txt\", \"w\")\n",
        "hi_nlp = stanza.Pipeline('hi')\n",
        "c = 0  # line counter\n",
        "\n",
        "pattern = \" कि \"  # added the spaces so that other words having \"ki\" are also not get used\n",
        "\n",
        "with open(\"team_1.txt\", \"r\") as file:  # opening the file team_1.txt to get its content\n",
        "    for line in file:  # parsing the file line by line\n",
        "        if re.search(pattern, line):  # if there is \"ki\" in the line\n",
        "            k = \"Sentence\" + \"<\" + str(c) + \">\" + \"\\n\" + line +\"\\n\"\n",
        "            \n",
        "            doc=hi_nlp(line)\n",
        "            line_list = line.split()  # tokenize the sentence into words and stores in the list line\n",
        "            line_list = [word for word in line_list if(word != \"ं\")]\n",
        "            # parser makes list inside list for the parsed words\n",
        "            parsed = parser.parse(line_list)\n",
        "\n",
        "            \n",
        "            word_before_ki = \"\"\n",
        "            for word in parsed:\n",
        "                \n",
        "                if(word[2] == \"कि\"):\n",
        "                    break\n",
        "                if(word[3] == \"VM\"):\n",
        "                    word_before_ki = word[2]\n",
        "\n",
        "            if(word_before_ki != \"\"):\n",
        "                output.write(\"----------------------------------\\n\")\n",
        "                output.write(k)\n",
        "                print(\"----------------------------------\\n\")\n",
        "                print(k)\n",
        "            \n",
        "                word_before_ki=word_before_ki.replace(\",\" , \"\") #just in case an extra, is added\n",
        "\n",
        "                for sentence in doc.sentences:\n",
        "                    for word in sentence.words:\n",
        "                        \n",
        "                        if (word.text==word_before_ki):\n",
        "                            oupt=word.text+\"->\" +word.lemma+\"\\n\"\n",
        "                            output.write(oupt)\n",
        "                            print(oupt) \n",
        "                            break           \n",
        "        c=c+1  # to keep the track of the line numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "c-biCMEYLBnr",
        "8hGW_Micxtg7",
        "q9pnxyQux-L6"
      ],
      "name": "CL_Project-1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
